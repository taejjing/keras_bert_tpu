{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras_bert_ver_bert_tpu.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"aBnI-oo48pWw","colab_type":"code","outputId":"d8b04b94-97a1-41d0-a192-eaf03b6ba653","executionInfo":{"status":"ok","timestamp":1559049223940,"user_tz":-540,"elapsed":24803,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","# from google.colab import drive\n","# drive.mount('/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QMntbvuzsCiE","colab_type":"code","outputId":"1ee1b233-fceb-4a8c-dd95-d987e8e4c2f9","executionInfo":{"status":"ok","timestamp":1559049241678,"user_tz":-540,"elapsed":15774,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["import os\n","\n","# @title Install Dependences\n","!pip install -q keras-bert\n","\n","# TF_KERAS must be added to environment variables in order to use TPU\n","os.environ['TF_KERAS'] = '1'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UNMmJJwZ8ptO","colab_type":"code","outputId":"fe563e99-6e3a-4333-b71e-847db8edf8c4","executionInfo":{"status":"ok","timestamp":1559049248672,"user_tz":-540,"elapsed":5347,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["# https://www.kaggle.com/taindow/bert-a-fine-tuning-example\n","import pprint\n","import time\n","import json\n","import sys\n","import collections\n","import csv\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import datetime\n","from tqdm import *\n","\n","\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n","if not 'bert_repo' in sys.path:\n","  sys.path += ['bert_repo']\n","\n","# import python modules defined by BERT\n","import run_classifier\n","import modeling\n","import optimization\n","import tokenization\n","\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensorflow version 1.13.1\n","Cloning into 'bert_repo'...\n","remote: Enumerating objects: 325, done.\u001b[K\n","remote: Total 325 (delta 0), reused 0 (delta 0), pack-reused 325\u001b[K\n","Receiving objects: 100% (325/325), 232.46 KiB | 3.10 MiB/s, done.\n","Resolving deltas: 100% (186/186), done.\n","TPU address is grpc://10.59.189.82:8470\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B1U6_ZoK_pA3","colab_type":"code","outputId":"a07bbca1-a0d8-4436-f5a1-c36955e64881","executionInfo":{"status":"ok","timestamp":1559049249712,"user_tz":-540,"elapsed":4348,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":233}},"source":["IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n","if IS_COLAB_BACKEND:\n","    from google.colab import auth\n","    auth.authenticate_user() # Authenticates the backend and also the TPU using your credentials so that they can access your private GCS buckets\n","    \n","    with tf.Session(TPU_ADDRESS) as session:\n","        print('TPU devices:')\n","        pprint.pprint(session.list_devices())\n","\n","        # Upload credentials to TPU.\n","        with open('/content/adc.json', 'r') as f:\n","            auth_info = json.load(f)\n","            tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","            # Now credentials are set for all future sessions on this TPU."],"execution_count":0,"outputs":[{"output_type":"stream","text":["TPU devices:\n","[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 16911102095813016144),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9032940763894176605),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9397770395000325313),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2799334155243662634),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9647770266160103276),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4093256256244460234),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9682279001468089904),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1376157206058696047),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13851991804207779683),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15491633251295249869),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 11336358804024724708)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RBXD-55u9w3l","colab_type":"code","outputId":"66e2f76f-eadf-489a-80ed-35bdd1528b2e","executionInfo":{"status":"ok","timestamp":1559049255775,"user_tz":-540,"elapsed":8215,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":179}},"source":["TASK = 'CoLA' #@param {type:\"string\"}\n","assert TASK in ('MRPC', 'CoLA'), 'Only (MRPC, CoLA) are demonstrated here.'\n","\n","BUCKET = 'tj-kaggle/toxic' #@param {type:\"string\"}\n","assert BUCKET, 'Must specify an existing GCS bucket name'\n","OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)\n","\n","# Available pretrained model checkpoints:\n","#   uncased_L-12_H-768_A-12    : uncased BERT base model\n","#   uncased_L-24_H-1024_A-16   : uncased BERT large model\n","#   cased_L-12_H-768_A-12      : cased BERT base model\n","#   cased_L-24_H-1024_A-16     : uncased BERT large model\n","\n","BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n","\n","\n","# Setup task specific model and TPU running config.\n","BERT_PRETRAINED_DIR = \"gs://{}/model/{}/\".format(BUCKET, BERT_MODEL)\n","print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n","!gsutil ls {BERT_PRETRAINED_DIR}\n","\n","CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n","BERT_CONFIG = modeling.BertConfig.from_json_file(CONFIG_FILE)\n","VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n","\n","checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n","\n","\n","IS_CASED_MODEL = False\n","BERT_STR = \"uncased\"\n","\n","if BERT_MODEL[:5] == \"cased\" :\n","    IS_CASED_MODEL = True\n","    BERT_STR = \"cased\"\n","\n","print(\"BERT_CASED : {}\".format(BERT_STR))\n","\n","# tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n","tokenizer = run_classifier.tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=IS_CASED_MODEL)\n","print(tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\"))\n","\n","print(\"Check the capital of word 'bert'..\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["***** BERT pretrained directory: gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/ *****\n","gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_config.json\n","gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n","gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n","gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\n","gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/vocab.txt\n","BERT_CASED : uncased\n","['[UNK]', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', '[UNK]', 'token', '##izer']\n","Check the capital of word 'bert'..\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ESYgXHki-2tV","colab_type":"code","outputId":"874d2fa2-5aa5-49db-fff0-c030c42b06d4","executionInfo":{"status":"ok","timestamp":1559049263727,"user_tz":-540,"elapsed":13449,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["!gsutil -m cp {BERT_PRETRAINED_DIR}* /content/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_config.json...\n","Copying gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_model.ckpt.index...\n","Copying gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001...\n","Copying gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/bert_model.ckpt.meta...\n","Copying gs://tj-kaggle/toxic/model/uncased_L-12_H-768_A-12/vocab.txt...\n","\\ [5/5 files][421.1 MiB/421.1 MiB] 100% Done                                    \n","Operation completed over 5 objects/421.1 MiB.                                    \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kbW2Mv7M3-un","colab_type":"code","colab":{}},"source":["DATA_PATH = '/content/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-U3K-HEBxWG","colab_type":"code","colab":{}},"source":["CONFIG_FILE = os.path.join(DATA_PATH, 'bert_config.json')\n","VOCAB_FILE = os.path.join(DATA_PATH, 'vocab.txt')\n","checkpoint_file = os.path.join(DATA_PATH, 'bert_model.ckpt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UljiOxlUGm6P","colab_type":"code","colab":{}},"source":["from keras_bert import load_trained_model_from_checkpoint"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ci6EbgipygR9","colab_type":"code","colab":{}},"source":["# @title Constants\n","\n","SEQ_LEN = 300\n","BATCH_SIZE = 128\n","EPOCHS = 2\n","LR = 2e-5\n","loss_weight = 3.2092275837114372"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kuuE1wb19v_9","colab_type":"code","outputId":"f113a2b1-16eb-4c8d-9b44-9835dbbaae57","executionInfo":{"status":"ok","timestamp":1559049298194,"user_tz":-540,"elapsed":37809,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":4769}},"source":["base_model = load_trained_model_from_checkpoint(CONFIG_FILE, checkpoint_file, training=True, trainable=True, seq_len=SEQ_LEN)\n","base_model.summary(line_length=120)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","________________________________________________________________________________________________________________________\n","Layer (type)                           Output Shape               Param #       Connected to                            \n","========================================================================================================================\n","Input-Token (InputLayer)               (None, 300)                0                                                     \n","________________________________________________________________________________________________________________________\n","Input-Segment (InputLayer)             (None, 300)                0                                                     \n","________________________________________________________________________________________________________________________\n","Embedding-Token (TokenEmbedding)       [(None, 300, 768), (30522, 23440896      Input-Token[0][0]                       \n","________________________________________________________________________________________________________________________\n","Embedding-Segment (Embedding)          (None, 300, 768)           1536          Input-Segment[0][0]                     \n","________________________________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)          (None, 300, 768)           0             Embedding-Token[0][0]                   \n","                                                                                Embedding-Segment[0][0]                 \n","________________________________________________________________________________________________________________________\n","Embedding-Position (PositionEmbedding) (None, 300, 768)           230400        Embedding-Token-Segment[0][0]           \n","________________________________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)            (None, 300, 768)           0             Embedding-Position[0][0]                \n","________________________________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalization)    (None, 300, 768)           1536          Embedding-Dropout[0][0]                 \n","________________________________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Embedding-Norm[0][0]                    \n","________________________________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-1-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Embedding-Norm[0][0]                    \n","                                                                                Encoder-1-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-1-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-1-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-1-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-1-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-1-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-1-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-1-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-1-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-1-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-1-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-1-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-2-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-1-FeedForward-Norm[0][0]        \n","                                                                                Encoder-2-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-2-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-2-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-2-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-2-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-2-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-2-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-2-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-2-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-2-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-2-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-2-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-3-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-2-FeedForward-Norm[0][0]        \n","                                                                                Encoder-3-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-3-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-3-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-3-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-3-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-3-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-3-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-3-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-3-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-3-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-3-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-3-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-4-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-3-FeedForward-Norm[0][0]        \n","                                                                                Encoder-4-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-4-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-4-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-4-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-4-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-4-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-4-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-4-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-4-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-4-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-4-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-4-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-5-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-4-FeedForward-Norm[0][0]        \n","                                                                                Encoder-5-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-5-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-5-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-5-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-5-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-5-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-5-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-5-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-5-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-5-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-5-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-5-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-6-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-5-FeedForward-Norm[0][0]        \n","                                                                                Encoder-6-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-6-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-6-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-6-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-6-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-6-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-6-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-6-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-6-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-6-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-6-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-6-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-7-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-6-FeedForward-Norm[0][0]        \n","                                                                                Encoder-7-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-7-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-7-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-7-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-7-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-7-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-7-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-7-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-7-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-7-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-7-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-7-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-8-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-7-FeedForward-Norm[0][0]        \n","                                                                                Encoder-8-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-8-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-8-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-8-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-8-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-8-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-8-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-8-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-8-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-8-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-8-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttention (Mult (None, 300, 768)           2362368       Encoder-8-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttention-Dropo (None, 300, 768)           0             Encoder-9-MultiHeadSelfAttention[0][0]  \n","________________________________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttention-Add ( (None, 300, 768)           0             Encoder-8-FeedForward-Norm[0][0]        \n","                                                                                Encoder-9-MultiHeadSelfAttention-Dropout\n","________________________________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttention-Norm  (None, 300, 768)           1536          Encoder-9-MultiHeadSelfAttention-Add[0][\n","________________________________________________________________________________________________________________________\n","Encoder-9-FeedForward (FeedForward)    (None, 300, 768)           4722432       Encoder-9-MultiHeadSelfAttention-Norm[0]\n","________________________________________________________________________________________________________________________\n","Encoder-9-FeedForward-Dropout (Dropout (None, 300, 768)           0             Encoder-9-FeedForward[0][0]             \n","________________________________________________________________________________________________________________________\n","Encoder-9-FeedForward-Add (Add)        (None, 300, 768)           0             Encoder-9-MultiHeadSelfAttention-Norm[0]\n","                                                                                Encoder-9-FeedForward-Dropout[0][0]     \n","________________________________________________________________________________________________________________________\n","Encoder-9-FeedForward-Norm (LayerNorma (None, 300, 768)           1536          Encoder-9-FeedForward-Add[0][0]         \n","________________________________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttention (Mul (None, 300, 768)           2362368       Encoder-9-FeedForward-Norm[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttention-Drop (None, 300, 768)           0             Encoder-10-MultiHeadSelfAttention[0][0] \n","________________________________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttention-Add  (None, 300, 768)           0             Encoder-9-FeedForward-Norm[0][0]        \n","                                                                                Encoder-10-MultiHeadSelfAttention-Dropou\n","________________________________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttention-Norm (None, 300, 768)           1536          Encoder-10-MultiHeadSelfAttention-Add[0]\n","________________________________________________________________________________________________________________________\n","Encoder-10-FeedForward (FeedForward)   (None, 300, 768)           4722432       Encoder-10-MultiHeadSelfAttention-Norm[0\n","________________________________________________________________________________________________________________________\n","Encoder-10-FeedForward-Dropout (Dropou (None, 300, 768)           0             Encoder-10-FeedForward[0][0]            \n","________________________________________________________________________________________________________________________\n","Encoder-10-FeedForward-Add (Add)       (None, 300, 768)           0             Encoder-10-MultiHeadSelfAttention-Norm[0\n","                                                                                Encoder-10-FeedForward-Dropout[0][0]    \n","________________________________________________________________________________________________________________________\n","Encoder-10-FeedForward-Norm (LayerNorm (None, 300, 768)           1536          Encoder-10-FeedForward-Add[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttention (Mul (None, 300, 768)           2362368       Encoder-10-FeedForward-Norm[0][0]       \n","________________________________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttention-Drop (None, 300, 768)           0             Encoder-11-MultiHeadSelfAttention[0][0] \n","________________________________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttention-Add  (None, 300, 768)           0             Encoder-10-FeedForward-Norm[0][0]       \n","                                                                                Encoder-11-MultiHeadSelfAttention-Dropou\n","________________________________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttention-Norm (None, 300, 768)           1536          Encoder-11-MultiHeadSelfAttention-Add[0]\n","________________________________________________________________________________________________________________________\n","Encoder-11-FeedForward (FeedForward)   (None, 300, 768)           4722432       Encoder-11-MultiHeadSelfAttention-Norm[0\n","________________________________________________________________________________________________________________________\n","Encoder-11-FeedForward-Dropout (Dropou (None, 300, 768)           0             Encoder-11-FeedForward[0][0]            \n","________________________________________________________________________________________________________________________\n","Encoder-11-FeedForward-Add (Add)       (None, 300, 768)           0             Encoder-11-MultiHeadSelfAttention-Norm[0\n","                                                                                Encoder-11-FeedForward-Dropout[0][0]    \n","________________________________________________________________________________________________________________________\n","Encoder-11-FeedForward-Norm (LayerNorm (None, 300, 768)           1536          Encoder-11-FeedForward-Add[0][0]        \n","________________________________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttention (Mul (None, 300, 768)           2362368       Encoder-11-FeedForward-Norm[0][0]       \n","________________________________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttention-Drop (None, 300, 768)           0             Encoder-12-MultiHeadSelfAttention[0][0] \n","________________________________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttention-Add  (None, 300, 768)           0             Encoder-11-FeedForward-Norm[0][0]       \n","                                                                                Encoder-12-MultiHeadSelfAttention-Dropou\n","________________________________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttention-Norm (None, 300, 768)           1536          Encoder-12-MultiHeadSelfAttention-Add[0]\n","________________________________________________________________________________________________________________________\n","Encoder-12-FeedForward (FeedForward)   (None, 300, 768)           4722432       Encoder-12-MultiHeadSelfAttention-Norm[0\n","________________________________________________________________________________________________________________________\n","Encoder-12-FeedForward-Dropout (Dropou (None, 300, 768)           0             Encoder-12-FeedForward[0][0]            \n","________________________________________________________________________________________________________________________\n","Encoder-12-FeedForward-Add (Add)       (None, 300, 768)           0             Encoder-12-MultiHeadSelfAttention-Norm[0\n","                                                                                Encoder-12-FeedForward-Dropout[0][0]    \n","________________________________________________________________________________________________________________________\n","Encoder-12-FeedForward-Norm (LayerNorm (None, 300, 768)           1536          Encoder-12-FeedForward-Add[0][0]        \n","________________________________________________________________________________________________________________________\n","MLM-Dense (Dense)                      (None, 300, 768)           590592        Encoder-12-FeedForward-Norm[0][0]       \n","________________________________________________________________________________________________________________________\n","MLM-Norm (LayerNormalization)          (None, 300, 768)           1536          MLM-Dense[0][0]                         \n","________________________________________________________________________________________________________________________\n","Extract (Extract)                      (None, 768)                0             Encoder-12-FeedForward-Norm[0][0]       \n","________________________________________________________________________________________________________________________\n","MLM-Sim (EmbeddingSimilarity)          (None, 300, 30522)         30522         MLM-Norm[0][0]                          \n","                                                                                Embedding-Token[0][1]                   \n","________________________________________________________________________________________________________________________\n","Input-Masked (InputLayer)              (None, 300)                0                                                     \n","________________________________________________________________________________________________________________________\n","NSP-Dense (Dense)                      (None, 768)                590592        Extract[0][0]                           \n","________________________________________________________________________________________________________________________\n","MLM (Masked)                           (None, 300, 30522)         0             MLM-Sim[0][0]                           \n","                                                                                Input-Masked[0][0]                      \n","________________________________________________________________________________________________________________________\n","NSP (Dense)                            (None, 2)                  1538          NSP-Dense[0][0]                         \n","========================================================================================================================\n","Total params: 109,943,612\n","Trainable params: 109,943,612\n","Non-trainable params: 0\n","________________________________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JN34KCM84Wtp","colab_type":"text"},"source":["# Generator"]},{"cell_type":"code","metadata":{"id":"S12csTFF4Z2m","colab_type":"code","outputId":"b5480097-ee4f-4c40-8693-be0b14615f1b","executionInfo":{"status":"ok","timestamp":1559049412456,"user_tz":-540,"elapsed":143625,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["! gsutil -m cp gs://tj-kaggle/toxic/data/X_seq_300.npy gs://tj-kaggle/toxic/data/y_aux* gs://tj-kaggle/toxic/data/y.npy  /content/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying gs://tj-kaggle/toxic/data/X_seq_300.npy...\n","/ [0 files][    0.0 B/  4.0 GiB]                                                \rCopying gs://tj-kaggle/toxic/data/y_aux_train.csv...\n","Copying gs://tj-kaggle/toxic/data/y.npy...\n","| [3/3 files][  4.1 GiB/  4.1 GiB] 100% Done  25.8 MiB/s ETA 00:00:00           \n","Operation completed over 3 objects/4.1 GiB.                                      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vbLH_K3JWy-R","colab_type":"code","colab":{}},"source":["# ! gsutil -m cp gs://tj-kaggle/toxic/keras_bert_2ep.h5  /content/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvXjT3s84fSx","colab_type":"code","colab":{}},"source":["import numpy as np\n","X = np.load('X_seq_300.npy')\n","y = np.load('y.npy')\n","\n","import pandas as pd \n","y_aux_train = pd.read_csv('y_aux_train.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyu6Zu8f4gWC","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import Sequence\n","\n","class DataGenerator(Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, data, labels, aux_labels, batch_size=128, x_dim=(220), y_dim=(2), y_aux_dim=(6), shuffle=True):\n","        'Initialization'\n","        self.x_dim = x_dim\n","        self.y_dim = y_dim\n","        self.y_aux_dim = y_aux_dim\n","        self.batch_size = batch_size\n","        self.labels = labels\n","        self.aux_labels = aux_labels\n","        self.data = data\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.data) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Generate data\n","        X, X_seg, X_mask, y, y_aux = self.__data_generation(indexes)\n","\n","        return [X, X_seg, X_mask], [y, y_aux]\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.data))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, indexes):\n","        'Generates data containing batch_size samples'\n","        # Initialization\n","        X = np.empty((self.batch_size, self.x_dim))\n","        y = np.empty((self.batch_size, self.y_dim), dtype=int)\n","        \n","        X = np.array([self.data[i] for i in indexes])\n","        X_seg = np.zeros((self.batch_size, self.x_dim))\n","        X_mask = np.ones((self.batch_size, self.x_dim))\n","        \n","        y = np.array([self.labels[i] for i in indexes])\n","        y_aux = np.array([self.aux_labels.loc[i,:] for i in indexes])\n","\n","\n","        return X, X_seg, X_mask, y, y_aux "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iF_oRkDoKjrF","colab_type":"code","colab":{}},"source":["def myGenerator(X, y, y_aux_train, batch_size, max_seq_len):\n","    x_dim = (max_seq_len)\n","    y_dim = (2)\n","    y_aux_dim = (6)\n","    \n","    # init\n","#     X = np.empty((batch_size, x_dim), dtype=np.int)\n","#     y = np.empty((batch_size, y_dim), dtype=np.float)\n","    data_size = len(X)\n","    remainder = data_size % batch_size\n","    indexes = np.arange(data_size)\n","    repeat = int(np.ceil(data_size / batch_size))\n","    \n","    # loop\n","    for index in range(0, repeat):\n","        i_batch = indexes[index*batch_size : (index+1)*batch_size]\n","        X_batch = np.array([X[i] for i in i_batch], dtype=np.int)\n","        y_batch = np.array([y[i] for i in i_batch], dtype=np.float)     \n","\n","        y_aux_batch = np.array([y_aux_train.loc[i,:] for i in i_batch], dtype=np.float)\n","        \n","        if index == repeat -1 : # last\n","            X_seg_batch = np.zeros((remainder, x_dim))\n","            X_mask_batch = np.ones((remainder, x_dim))\n","            \n","        else :\n","            X_seg_batch = np.zeros((batch_size, x_dim))\n","            X_mask_batch = np.ones((batch_size, x_dim))\n","\n","        yield [X_batch, X_seg_batch, X_mask_batch], [y_batch, y_aux_batch]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWjGjD3TlLdd","colab_type":"code","colab":{}},"source":["y_aux_train = y_aux_train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeV553ZE4gUE","colab_type":"code","colab":{}},"source":["params = {'batch_size' : BATCH_SIZE, \n","          'x_dim' :  (SEQ_LEN),\n","          'y_dim' : (2),\n","          'y_aux_dim' : (6),\n","          'shuffle' : True}\n","training_generator = DataGenerator(X, y, y_aux_train, **params)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGadCzmB4gR3","colab_type":"code","colab":{}},"source":["# training_generator = myGenerator(X, y, y_aux_train, BATCH_SIZE, SEQ_LEN)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E40a_pjVExRw","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.losses import binary_crossentropy\n","from tensorflow.keras import backend as K\n","\n","def custom_loss(y_true, y_pred):\n","    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-Yj1ree1QEl","colab_type":"code","outputId":"7f62a9b6-3bdd-413e-d33d-1c23968b8b63","executionInfo":{"status":"ok","timestamp":1559023592770,"user_tz":-540,"elapsed":161025,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["base_model.get_layer('NSP-Dense').output"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'NSP-Dense/Tanh:0' shape=(?, 768) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"mDYzuY7v2j1T","colab_type":"code","outputId":"d3cd2d6a-2b2b-4f0b-d759-94ec981adb76","executionInfo":{"status":"ok","timestamp":1559049423716,"user_tz":-540,"elapsed":60549,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":4517}},"source":["# from tensorflow.python import keras\n","from tensorflow.python import keras\n","from keras_bert import calc_train_steps\n","\n","extract = base_model.get_layer('Extract').output\n","# target_layer = keras.layers.Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02), name='target_layer')(extract)\n","# aux_layer = keras.layers.Dense(6, activation='sigmoid', kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),  name='aux_layer')(extract)\n","\n","final_layer = keras.layers.Dense(256, activation='relu')(extract)\n","final_layer = keras.layers.Dropout(0.25)(final_layer)\n","\n","target_layer = keras.layers.Dense(1, activation='sigmoid', name='target_layer')(final_layer)\n","aux_layer = keras.layers.Dense(6, activation='sigmoid', name='aux_layer')(final_layer)\n","\n","# decay_steps, warmup_steps = calc_train_steps(\n","#     y.shape[0],\n","#     batch_size=BATCH_SIZE,\n","#     epochs=EPOCHS,\n","# )\n","\n","\n","model = keras.models.Model(inputs=base_model.input, outputs=[target_layer, aux_layer])\n","# model.compile(\n","#     optimizer = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n","#     loss = [custom_loss, 'binary_crossentropy'],\n","#     loss_weights = [loss_weight, 1.0]\n","# )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Input-Token (InputLayer)        (None, 300)          0                                            \n","__________________________________________________________________________________________________\n","Input-Segment (InputLayer)      (None, 300)          0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (TokenEmbedding [(None, 300, 768), ( 23440896    Input-Token[0][0]                \n","__________________________________________________________________________________________________\n","Embedding-Segment (Embedding)   (None, 300, 768)     1536        Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)   (None, 300, 768)     0           Embedding-Token[0][0]            \n","                                                                 Embedding-Segment[0][0]          \n","__________________________________________________________________________________________________\n","Embedding-Position (PositionEmb (None, 300, 768)     230400      Embedding-Token-Segment[0][0]    \n","__________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)     (None, 300, 768)     0           Embedding-Position[0][0]         \n","__________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalizat (None, 300, 768)     1536        Embedding-Dropout[0][0]          \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 300, 768)     2362368     Embedding-Norm[0][0]             \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-1-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 300, 768)     0           Embedding-Norm[0][0]             \n","                                                                 Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-1-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-1-MultiHeadSelfAttention-\n","                                                                 Encoder-1-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-2-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n","                                                                 Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-2-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-2-MultiHeadSelfAttention-\n","                                                                 Encoder-2-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-3-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n","                                                                 Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-3-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-3-MultiHeadSelfAttention-\n","                                                                 Encoder-3-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-4-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n","                                                                 Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-4-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-4-MultiHeadSelfAttention-\n","                                                                 Encoder-4-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-5-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n","                                                                 Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-5-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-5-MultiHeadSelfAttention-\n","                                                                 Encoder-5-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-6-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n","                                                                 Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-6-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-6-MultiHeadSelfAttention-\n","                                                                 Encoder-6-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-7-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n","                                                                 Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-7-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-7-MultiHeadSelfAttention-\n","                                                                 Encoder-7-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-8-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n","                                                                 Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-8-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-8-MultiHeadSelfAttention-\n","                                                                 Encoder-8-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 300, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-9-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 300, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n","                                                                 Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 300, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward (FeedForw (None, 300, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Dropout ( (None, 300, 768)     0           Encoder-9-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Add (Add) (None, 300, 768)     0           Encoder-9-MultiHeadSelfAttention-\n","                                                                 Encoder-9-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Norm (Lay (None, 300, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 300, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 300, 768)     0           Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 300, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 300, 768)     1536        Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward (FeedFor (None, 300, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Dropout  (None, 300, 768)     0           Encoder-10-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Add (Add (None, 300, 768)     0           Encoder-10-MultiHeadSelfAttention\n","                                                                 Encoder-10-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Norm (La (None, 300, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 300, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 300, 768)     0           Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 300, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 300, 768)     1536        Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward (FeedFor (None, 300, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Dropout  (None, 300, 768)     0           Encoder-11-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Add (Add (None, 300, 768)     0           Encoder-11-MultiHeadSelfAttention\n","                                                                 Encoder-11-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Norm (La (None, 300, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 300, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 300, 768)     0           Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 300, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 300, 768)     1536        Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward (FeedFor (None, 300, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Dropout  (None, 300, 768)     0           Encoder-12-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Add (Add (None, 300, 768)     0           Encoder-12-MultiHeadSelfAttention\n","                                                                 Encoder-12-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Norm (La (None, 300, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 256)          196864      Extract[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","target_layer (Dense)            (None, 1)            257         dropout[0][0]                    \n","__________________________________________________________________________________________________\n","aux_layer (Dense)               (None, 6)            1542        dropout[0][0]                    \n","==================================================================================================\n","Total params: 108,927,495\n","Trainable params: 108,927,495\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XVRlA7jEAGKT","colab_type":"code","colab":{}},"source":["# class AdamWarmup(tf.train.AdamOptimizer):\n","#     def __init__(self, decay_steps, warmup_steps, min_lr=0.0,\n","#                  lr=0.001, beta_1=0.9, beta_2=0.999,\n","#                  epsilon=None, kernel_weight_decay=0., bias_weight_decay=0.,\n","#                  amsgrad=False, **kwargs):\n","#         super(AdamWarmup, self).__init__(**kwargs)\n","#         with K.name_scope(self.__class__.__name__):\n","#             self.decay_steps = K.variable(decay_steps, name='decay_steps')\n","#             self.warmup_steps = K.variable(warmup_steps, name='warmup_steps')\n","#             self.min_lr = K.variable(min_lr, name='min_lr')\n","#             self.iterations = K.variable(0, dtype='int64', name='iterations')\n","#             self.lr = K.variable(lr, name='lr')\n","#             self.beta_1 = K.variable(beta_1, name='beta_1')\n","#             self.beta_2 = K.variable(beta_2, name='beta_2')\n","#             self.kernel_weight_decay = K.variable(kernel_weight_decay, name='kernel_weight_decay')\n","#             self.bias_weight_decay = K.variable(bias_weight_decay, name='bias_weight_decay')\n","#         if epsilon is None:\n","#             epsilon = K.epsilon()\n","#         self.epsilon = epsilon\n","#         self.initial_kernel_weight_decay = kernel_weight_decay\n","#         self.initial_bias_weight_decay = bias_weight_decay\n","#         self.amsgrad = amsgrad\n","\n","#     def get_updates(self, loss, params):\n","#         grads = self.get_gradients(loss, params)\n","#         self.updates = [K.update_add(self.iterations, 1)]\n","\n","#         t = K.cast(self.iterations, K.floatx()) + 1\n","\n","#         lr = K.switch(\n","#             t <= self.warmup_steps,\n","#             self.lr * (t / self.warmup_steps),\n","#             self.lr * (1.0 - K.minimum(t, self.decay_steps) / self.decay_steps),\n","#         )\n","\n","#         lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n","#                      (1. - K.pow(self.beta_1, t)))\n","\n","#         ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","#         vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","#         if self.amsgrad:\n","#             vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","#         else:\n","#             vhats = [K.zeros(1) for _ in params]\n","#         self.weights = [self.iterations] + ms + vs + vhats\n","\n","#         for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n","#             m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n","#             v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n","#             if self.amsgrad:\n","#                 vhat_t = K.maximum(vhat, v_t)\n","#                 p_t = m_t / (K.sqrt(vhat_t) + self.epsilon)\n","#                 self.updates.append(K.update(vhat, vhat_t))\n","#             else:\n","#                 p_t = m_t / (K.sqrt(v_t) + self.epsilon)\n","\n","#             if 'bias' in p.name:\n","#                 if self.initial_bias_weight_decay > 0.0:\n","#                     p_t += self.bias_weight_decay * p\n","#             else:\n","#                 if self.initial_kernel_weight_decay > 0.0:\n","#                     p_t += self.kernel_weight_decay * p\n","#             p_t = p - lr_t * p_t\n","\n","#             self.updates.append(K.update(m, m_t))\n","#             self.updates.append(K.update(v, v_t))\n","#             new_p = p_t\n","\n","#             if getattr(p, 'constraint', None) is not None:\n","#                 new_p = p.constraint(new_p)\n","\n","#             self.updates.append(K.update(p, new_p))\n","#         return self.updates\n","\n","#     def get_config(self):\n","#         config = {\n","#             'decay_steps': float(K.get_value(self.decay_steps)),\n","#             'warmup_steps': float(K.get_value(self.warmup_steps)),\n","#             'min_lr': float(K.get_value(self.min_lr)),\n","#             'lr': float(K.get_value(self.lr)),\n","#             'beta_1': float(K.get_value(self.beta_1)),\n","#             'beta_2': float(K.get_value(self.beta_2)),\n","#             'epsilon': self.epsilon,\n","#             'kernel_weight_decay': float(K.get_value(self.kernel_weight_decay)),\n","#             'bias_weight_decay': float(K.get_value(self.bias_weight_decay)),\n","#             'amsgrad': self.amsgrad,\n","#         }\n","#         base_config = super(AdamWarmup, self).get_config()\n","#         return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GOj7nWw7-oWx","colab_type":"code","colab":{}},"source":["# @title Initialize Variables\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","sess = K.get_session()\n","uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n","init_op = tf.variables_initializer(\n","    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",")\n","sess.run(init_op)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDn31jx1-172","colab_type":"code","outputId":"c3a30b84-2de4-4ca2-85e7-e6b4c583bd8b","executionInfo":{"status":"ok","timestamp":1559049614383,"user_tz":-540,"elapsed":37243,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["import tensorflow as tf\n","from keras_bert import get_custom_objects\n","\n","tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","strategy = tf.contrib.tpu.TPUDistributionStrategy(\n","    tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",")\n","\n","with tf.keras.utils.custom_object_scope(get_custom_objects()):\n","    tpu_model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n","    tpu_model.compile(\n","#         optimizer = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n","        optimizer = tf.train.AdamOptimizer(learning_rate=LR),\n","        loss = [custom_loss, 'binary_crossentropy'],\n","        loss_weights = [loss_weight, 1.0]\n","    )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Querying Tensorflow master (grpc://10.59.189.82:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16911102095813016144)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9032940763894176605)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9397770395000325313)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2799334155243662634)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9647770266160103276)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4093256256244460234)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9682279001468089904)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1376157206058696047)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13851991804207779683)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15491633251295249869)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 11336358804024724708)\n","WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rO9RzwoxlygB","colab_type":"code","colab":{}},"source":["filepath = \"keras-bert-datagen-{epoch:02d}.h5\"\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, save_weights_only=False)\n","callbacks_list = [checkpoint]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjT8DLGwjMXD","colab_type":"code","outputId":"78b84004-4861-4eb6-cadf-b7693bdf9d6e","executionInfo":{"status":"ok","timestamp":1558881232664,"user_tz":-540,"elapsed":179213,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["int(np.ceil(X.shape[0] / BATCH_SIZE)) - 1"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14101"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"mIpSmOzF_H8m","colab_type":"code","outputId":"fe6b957d-c5f0-41bd-9025-c4992b010a62","executionInfo":{"status":"ok","timestamp":1559033437671,"user_tz":-540,"elapsed":9821141,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["with tf.keras.utils.custom_object_scope(get_custom_objects()):\n","    tpu_model.fit_generator(\n","        training_generator, \n","        epochs=2,\n","        callbacks=callbacks_list\n","    )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","14099/14100 [============================>.] - ETA: 0s - loss: 0.2901 - target_layer_loss: 0.0784 - aux_layer_loss: 0.0784\n","Epoch 00001: saving model to keras-bert-datagen-01.h5\n","INFO:tensorflow:Copying TPU weights to the CPU\n","WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","14100/14100 [==============================] - 9715s 689ms/step - loss: 0.2901 - target_layer_loss: 0.0784 - aux_layer_loss: 0.0784\n","Epoch 2/2\n"," 5884/14100 [===========>..................] - ETA: 1:34:32 - loss: 0.2739 - target_layer_loss: 0.0751 - aux_layer_loss: 0.0751"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sp-uiuVtmA7Y","colab_type":"code","colab":{}},"source":["!gsutil -m cp /content/*datagen* gs://tj-kaggle/toxic/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"erG_6owvicdW","colab_type":"code","outputId":"96679519-3b2a-48c8-aa38-aaa3dea6a5f0","executionInfo":{"status":"ok","timestamp":1559033595908,"user_tz":-540,"elapsed":9955562,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["fileName = '/content/bert_add_dense_adam_preprocessed_seq300.h5'\n","tpu_model.save_weights(fileName)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Copying TPU weights to the CPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bcRJLnzEimAq","colab_type":"code","outputId":"23cebee7-6005-479f-d5cd-deafd765724f","executionInfo":{"status":"ok","timestamp":1559033606504,"user_tz":-540,"elapsed":9964608,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":307}},"source":["!gsutil -m cp {fileName} gs://tj-kaggle/toxic/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying file:///content/bert_add_dense_adam_preprocessed_seq300.h5 [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run\n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","|\n","Operation completed over 1 objects/415.8 MiB.                                    \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"52LJJyH3A4Np","colab_type":"code","colab":{}},"source":["# with tf.keras.utils.custom_object_scope(get_custom_objects()):\n","#     tpu_model.fit(\n","#         [X, X_seg, X_mask],\n","#         [y, y_aux_train.values],\n","#         epochs=EPOCHS,\n","#         batch_size=BATCH_SIZE,\n","#     )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPz3YOT5rlIx","colab_type":"text"},"source":["# Predfict , make submission"]},{"cell_type":"code","metadata":{"id":"Sd-rR1Bcrk_1","colab_type":"code","outputId":"f0ea4bf3-d3b1-4edb-ed13-6a87b1852ce6","executionInfo":{"status":"ok","timestamp":1559060669062,"user_tz":-540,"elapsed":259288,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":703}},"source":["! gsutil -m cp gs://tj-kaggle/toxic/data/X_test.npy gs://tj-kaggle/toxic/data/*submission* /content/\n","        \n","X_test = np.load('X_test.npy')\n","X_seg_input = np.zeros((X_test.shape[0], SEQ_LEN))\n","X_mask_input = np.ones((X_test.shape[0], SEQ_LEN))\n","with tf.keras.utils.custom_object_scope(get_custom_objects()):\n","    history = tpu_model.predict([X_test[:32], X_seg_input[:32], X_mask_input[:32]], batch_size=32, verbose=1)\n","submission = pd.read_csv('sample_submission.csv', index_col='id')[:32]\n","submission['prediction'] = history[0]\n","submission.to_csv('submission.csv')\n","\n","print(submission.head(20))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(16, 300), dtype=tf.float32, name='Input-Token_20'), TensorSpec(shape=(16, 300), dtype=tf.float32, name='Input-Segment_20'), TensorSpec(shape=(16, 300), dtype=tf.float32, name='Input-Masked_20')]\n","INFO:tensorflow:Overriding default placeholder.\n","INFO:tensorflow:Remapping placeholder for Input-Token\n","INFO:tensorflow:Remapping placeholder for Input-Segment\n","INFO:tensorflow:Remapping placeholder for Input-Masked\n","INFO:tensorflow:Started compiling\n","INFO:tensorflow:Finished compiling. Time elapsed: 45.296220779418945 secs\n","97280/97320 [============================>.] - ETA: 0sINFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(5, 300), dtype=tf.float32, name='Input-Token_20'), TensorSpec(shape=(5, 300), dtype=tf.float32, name='Input-Segment_20'), TensorSpec(shape=(5, 300), dtype=tf.float32, name='Input-Masked_20')]\n","INFO:tensorflow:Overriding default placeholder.\n","INFO:tensorflow:Remapping placeholder for Input-Token\n","INFO:tensorflow:Remapping placeholder for Input-Segment\n","INFO:tensorflow:Remapping placeholder for Input-Masked\n","INFO:tensorflow:Started compiling\n","INFO:tensorflow:Finished compiling. Time elapsed: 36.846081018447876 secs\n","97320/97320 [==============================] - 234s 2ms/sample\n","         prediction\n","id                 \n","7000000    0.002717\n","7000001    0.001372\n","7000002    0.004555\n","7000003    0.002167\n","7000004    0.947372\n","7000005    0.001480\n","7000006    0.003882\n","7000007    0.002995\n","7000008    0.001788\n","7000009    0.002398\n","7000010    0.002095\n","7000011    0.416767\n","7000012    0.003123\n","7000013    0.001864\n","7000014    0.002769\n","7000015    0.001478\n","7000016    0.284794\n","7000017    0.002439\n","7000018    0.348055\n","7000019    0.019562\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IPXXwdfzXkiC","colab_type":"text"},"source":["## if you want to load checkpoint.."]},{"cell_type":"code","metadata":{"id":"peOy9K5JXkCi","colab_type":"code","colab":{}},"source":["model.load_weights('/content/keras-bert-datagen-01.h5')\n","model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer=tf.train.AdamOptimizer(learning_rate=LR))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ict8DOyRVo9","colab_type":"code","outputId":"f85d8925-52fd-4d32-ae9d-30bcbaa34157","executionInfo":{"status":"ok","timestamp":1559061181630,"user_tz":-540,"elapsed":4529,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":431}},"source":["history = model.predict([X_test[:20], X_seg_input[:20], X_mask_input[:20]], batch_size = 20, verbose=1)\n","submission = pd.read_csv('sample_submission.csv', index_col='id')[:20]\n","\n","submission['prediction'] = history[0]\n","submission.to_csv('submission.csv')\n","\n","print(submission.head(20))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r20/20 [==============================] - 2s 98ms/sample\n","         prediction\n","id                 \n","7000000    0.002732\n","7000001    0.001375\n","7000002    0.004579\n","7000003    0.002157\n","7000004    0.947316\n","7000005    0.001487\n","7000006    0.003871\n","7000007    0.003015\n","7000008    0.001783\n","7000009    0.002385\n","7000010    0.002080\n","7000011    0.417077\n","7000012    0.003129\n","7000013    0.001860\n","7000014    0.002776\n","7000015    0.001485\n","7000016    0.285142\n","7000017    0.002427\n","7000018    0.347868\n","7000019    0.019464\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Ek63qyEEp4N","colab_type":"code","outputId":"ad1b894a-2254-43e0-ddd9-121ef10b6899","executionInfo":{"status":"ok","timestamp":1558621689004,"user_tz":-540,"elapsed":17794,"user":{"displayName":"김태진","photoUrl":"","userId":"00641355406885108566"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tpu_model.save_weights('/content/keras_bert_ep{}.h5'.format(4))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Copying TPU weights to the CPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k22qkS4wWFBs","colab_type":"code","colab":{}},"source":["!gsutil -m cp /content/keras-bert*.h5 gs://tj-kaggle/toxic/"],"execution_count":0,"outputs":[]}]}